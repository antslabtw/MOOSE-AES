{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0670f72-1bee-4b9c-9dec-c32072d7a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "with open (f\"/home/tsaibw/Multi_scale/pre_process/hand_crafted_v3.csv\", mode='r', encoding='utf-8') as f:\n",
    "    file = csv.reader(f, delimiter=',')\n",
    "    hand_craft = list(file)\n",
    "    \n",
    "with open(f\"/home/tsaibw/Multi_scale/pre_process/allreadability.pickle\", \"rb\") as f:\n",
    "    readability = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61303625-9fba-4b19-8824-af4f140e78bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.47361136 0.51242417 0.41745715 0.56747815 0.39605389\n",
      " 0.51401774 0.54984141 0.40764331 0.24500106 0.40405317 0.38893406\n",
      " 0.44884299 0.24285714 0.32036852 0.39883753 0.374211   0.42465753\n",
      " 0.46045198 0.24285714 0.         0.31052632 0.1969697  0.29918033\n",
      " 0.20408163 0.14285714 0.31818182 0.39130435 0.47008547 0.07317073\n",
      " 0.08       0.28571429 0.         0.33333333 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(readability[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b21046-9d9a-492d-824e-be7882d9b177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunk_sizes = [90,30,130,10]\n",
    "def process_bert_inputs(inputs):\n",
    "    valid_lengths = []\n",
    "\n",
    "    for batch in inputs:\n",
    "        valid_length = 0\n",
    "\n",
    "        for idxx, element in enumerate(batch):\n",
    "            input_id, token_type, mask = element\n",
    "\n",
    "            # 檢查input_id是否為全0\n",
    "            if not torch.all(input_id == 0):\n",
    "                valid_length += 1\n",
    "            else:\n",
    "                input_id[0] = 101  # CLS token\n",
    "                input_id[1] = 102  # SEP token\n",
    "                token_type[:] = 0\n",
    "                mask[:2] = 1\n",
    "                batch[idxx] = torch.stack((input_id, token_type, mask))\n",
    "\n",
    "        valid_lengths.append(valid_length)\n",
    "\n",
    "    return inputs, valid_lengths\n",
    "\n",
    "\n",
    "def adds(data, mode, epoch):\n",
    "    attribute = ['score',\n",
    "            'content',\n",
    "            'organization',\n",
    "            'word_choice',\n",
    "            'sentence_fluency',\n",
    "            'conventions',\n",
    "            'prompt_adherence',\n",
    "            'language',\n",
    "            'narrativity']\n",
    "    essay_id, prompt_id, score, text, craft, read, inputs_chunked = [], [], [], [], [], [], []\n",
    "    \n",
    "    for i in data:\n",
    "        essay_id.append(int(i['essay_id']))\n",
    "        prompt_id.append(int(i['prompt_id']))\n",
    "        text.append(i['content_text'])\n",
    "        att = [-1]*9\n",
    "        for j in i.keys():\n",
    "            if j in attribute:\n",
    "                idx = attribute.index(j)\n",
    "                att[idx] = int(i[j])\n",
    "        score.append(att)\n",
    "        \n",
    "        id_ = i['essay_id']\n",
    "        for j in hand_craft:\n",
    "            if j[0] == id_:\n",
    "                craft.append(j[2:])\n",
    "                break\n",
    "    \n",
    "        for j in readability:\n",
    "            if int(j[0]) == int(id_):\n",
    "                read.append(list(j[1:]))\n",
    "                break\n",
    "\n",
    "    labels = list(score)\n",
    "    test_documents = list(text)\n",
    "    ids = list(prompt_id)\n",
    "    # normalized\n",
    "    scale_score = get_scaled_down_scores(labels, ids)\n",
    "    \n",
    "    inputs_single, _ = encode_documents(test_documents, tokenizer, max_input_length=512)\n",
    "    # segment_encode\n",
    "    for chunk_size in chunk_sizes:\n",
    "        input_enc_chunk, _ = encode_documents(test_documents, tokenizer, max_input_length=chunk_size)\n",
    "        inputs_chunked.append(input_enc_chunk)\n",
    "    \n",
    "    \n",
    "    # 處理輸入\n",
    "    processed_inputs, valid_lengths = [], []\n",
    "    for batch in inputs_chunked:\n",
    "        processed_batch, lengths = process_bert_inputs(batch)\n",
    "        processed_inputs.append(processed_batch)\n",
    "        valid_lengths.append(lengths)\n",
    "\n",
    "    # 輸出處理結果\n",
    "    print(f\"Valid lengths : {valid_lengths}\")\n",
    "    out = {\n",
    "        'inputs_single': inputs_single,\n",
    "        'inputs_chunked': processed_inputs,\n",
    "        'scaled_score': scale_score,\n",
    "        'prompt_id': ids,\n",
    "        'valid_length': valid_lengths,\n",
    "        'hand_craft': craft,\n",
    "        'readability': read,\n",
    "        'essay_id': essay_id\n",
    "    }\n",
    "    # data2 = (inputs_single, processed_inputs, labels, ids, valid_lengths)\n",
    "    with open(f\"/home/tsaibw/Multi_scale/dataset/new_{mode}/encode_prompt_{epoch}.pkl\", 'wb') as f:\n",
    "        pickle.dump(out, f)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20ef2ffa-e4b2-4dd1-9aca-f17ce8a4037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"/home/tsaibw/Multi_scale/dataset/new_train/encode_prompt_1.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09ba4cb0-0a95-46ad-afd6-1c2509bffe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in data.keys():\n",
    "#     print(i, data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c554d0-cd12-4274-8b14-a316494fc344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data set\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from scale import  get_scaled_down_scores\n",
    "from tqdm import tqdm \n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from data.encode import encode_documents\n",
    "\n",
    "random.seed(0)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for i in tqdm(range(1, 9)):\n",
    "    with open(f\"/home/tsaibw/Multi_scale/pre_process/cross_prompt_attributes/{i}/train.pk\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    with open(f\"/home/tsaibw/Multi_scale/pre_process/cross_prompt_attributes/{i}/dev.pk\", \"rb\") as f:\n",
    "        dev_data = pickle.load(f)\n",
    "    with open(f\"/home/tsaibw/Multi_scale/pre_process/cross_prompt_attributes/{i}/test.pk\", \"rb\") as f:\n",
    "        test_data = pickle.load(f)\n",
    "    \n",
    "    adds(data, 'train', i)\n",
    "    adds(dev_data, 'dev', i)\n",
    "    adds(test_data, 'test', i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db717ba1-d4e2-4eaf-91ca-1c98ebf2c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "%cd '/content/drive/MyDrive/hw_ner'\n",
    "max_len = 1024\n",
    "for j in range(1, 9):\n",
    "  ids = []\n",
    "  texts = []\n",
    "  labels = []\n",
    "  for i in range (1, 9):\n",
    "    if j == i:\n",
    "      continue\n",
    "    with open(f\"/content/drive/MyDrive/hw_ner/ASAP/dataset/Train_prompt/train_prompt_{i}.txt\") as file:\n",
    "      for lines in file:\n",
    "        line = lines.strip()\n",
    "        line_vec = line.split(\"\\t\")\n",
    "        if len(line_vec) == 3:\n",
    "            ids.append(i)\n",
    "            if len(line_vec[1].split(\" \")) >= max_len:\n",
    "                line_vec[1] = \" \".join(line_vec[1].split(\" \")[0:max_len])\n",
    "            texts.append(line_vec[1])\n",
    "            labels.append(float(line_vec[2]))\n",
    "  data_to_save = (ids, texts, labels)\n",
    "  with open(f\"/multi_scale/train/cross_prompt_p{j}.pkl\", 'wb') as f:\n",
    "      pickle.dump(data_to_save, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env ProTact",
   "language": "python",
   "name": "protact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
